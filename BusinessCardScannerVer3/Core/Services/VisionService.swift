//
//  VisionService.swift
//  BusinessCardScannerVer3
//
//  Vision Framework OCR æœå‹™
//

import UIKit
import Vision
import Combine

/// OCR è­˜åˆ¥çµæœ
struct OCRResult {
    let recognizedText: String
    let confidence: Float
    let boundingBoxes: [TextBoundingBox]
    let processingTime: TimeInterval
}

/// æ–‡å­—é‚Šç•Œæ¡†è³‡è¨Š
struct TextBoundingBox {
    let text: String
    let confidence: Float
    let boundingBox: CGRect
    let topCandidates: [String]
}

/// OCR éŒ¯èª¤é¡å‹
enum OCRError: LocalizedError {
    case imageProcessingFailed
    case noTextFound
    case visionRequestFailed(Error)
    case invalidImage
    
    var errorDescription: String? {
        switch self {
        case .imageProcessingFailed:
            return "åœ–ç‰‡è™•ç†å¤±æ•—"
        case .noTextFound:
            return "æœªæ‰¾åˆ°æ–‡å­—"
        case .visionRequestFailed(let error):
            return "Vision è­˜åˆ¥å¤±æ•—: \(error.localizedDescription)"
        case .invalidImage:
            return "ç„¡æ•ˆçš„åœ–ç‰‡"
        }
    }
}

/// Vision Framework OCR æœå‹™
class VisionService {
    
    // MARK: - Singleton
    
    static let shared = VisionService()
    private init() {}
    
    // MARK: - Properties
    
    /// OCR è«‹æ±‚é…ç½®
    private var textRecognitionRequest: VNRecognizeTextRequest {
        let request = VNRecognizeTextRequest()
        
        // è¨­å®šè­˜åˆ¥ç­‰ç´šï¼ˆç²¾ç¢ºåº¦ vs é€Ÿåº¦ï¼‰
        request.recognitionLevel = .accurate
        
        // æª¢æŸ¥ä¸¦è¨­å®šæ”¯æ´çš„èªè¨€
        let supportedLanguages = try? VNRecognizeTextRequest.supportedRecognitionLanguages(for: .accurate, revision: VNRecognizeTextRequestRevision1)
        print("ğŸŒ VisionService: ç³»çµ±æ”¯æ´çš„èªè¨€: \(supportedLanguages ?? [])")
        
        // ä¿®æ­£ï¼šä½¿ç”¨ Ver2 çš„æ¢ä»¶å¼èªè¨€è¨­å®šé‚è¼¯
        if #available(iOS 16.0, *) {
            // iOS 16+ æ”¯æ´æ›´å¤šèªè¨€
            request.recognitionLanguages = ["zh-Hant", "en-US"] // ç¹é«”ä¸­æ–‡å’Œè‹±æ–‡
            print("ğŸ¯ iOS 16+ ä½¿ç”¨èªè¨€: [\"zh-Hant\", \"en-US\"]")
        } else {
            // iOS 15 åŠä»¥ä¸‹ç‰ˆæœ¬ä½¿ç”¨è‹±æ–‡
            request.recognitionLanguages = ["en-US"]
            print("ğŸ¯ iOS 15- ä½¿ç”¨èªè¨€: [\"en-US\"]")
        }
        
        // ä½¿ç”¨èªè¨€æ ¡æ­£
        request.usesLanguageCorrection = true
        
        // è¨­å®šæœ€å°æ–‡å­—é«˜åº¦ï¼ˆé™ä½é–¾å€¼ä»¥æé«˜å°å­—è­˜åˆ¥ï¼‰
        request.minimumTextHeight = 0.02
        
        // è¨­å®šè‡ªå‹•è­˜åˆ¥èªè¨€
        request.automaticallyDetectsLanguage = true
        
        return request
    }
    
    // MARK: - Public Methods
    
    /// å°åœ–ç‰‡åŸ·è¡Œ OCR æ–‡å­—è­˜åˆ¥
    /// - Parameters:
    ///   - image: è¦è­˜åˆ¥çš„åœ–ç‰‡
    ///   - completion: å®Œæˆå›èª¿ (Result<OCRResult, OCRError>)
    func recognizeText(from image: UIImage, completion: @escaping (Result<OCRResult, OCRError>) -> Void) {
        let startTime = CFAbsoluteTimeGetCurrent()
        
        // å…ˆæ­£è¦åŒ–åœ–ç‰‡æ–¹å‘ï¼ˆåƒè€ƒ Ver2ï¼‰
        guard let normalizedImage = image.normalizeOrientation(),
              let cgImage = normalizedImage.cgImage else {
            completion(.failure(.invalidImage))
            return
        }
        
        // åœ¨èƒŒæ™¯åŸ·è¡Œç·’è™•ç†
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }
            
            let request = self.textRecognitionRequest
            // ä½¿ç”¨æ­£è¦åŒ–å¾Œçš„åœ–ç‰‡ï¼Œæ–¹å‘ç‚º .up
            let handler = VNImageRequestHandler(cgImage: cgImage, orientation: .up, options: [:])
            
            do {
                try handler.perform([request])
                
                guard let observations = request.results else {
                    DispatchQueue.main.async {
                        completion(.failure(.noTextFound))
                    }
                    return
                }
                
                let result = self.processObservations(observations, processingTime: CFAbsoluteTimeGetCurrent() - startTime)
                
                DispatchQueue.main.async {
                    completion(.success(result))
                }
                
            } catch {
                DispatchQueue.main.async {
                    completion(.failure(.visionRequestFailed(error)))
                }
            }
        }
    }
    
    /// å°åœ–ç‰‡åŸ·è¡Œ OCR æ–‡å­—è­˜åˆ¥ (Combine ç‰ˆæœ¬)
    /// - Parameter image: è¦è­˜åˆ¥çš„åœ–ç‰‡
    /// - Returns: OCR çµæœçš„ Publisher
    func recognizeText(from image: UIImage) -> AnyPublisher<OCRResult, OCRError> {
        return Future { [weak self] promise in
            self?.recognizeText(from: image) { result in
                promise(result)
            }
        }
        .eraseToAnyPublisher()
    }
    
    // MARK: - Private Methods
    
    /// è™•ç† Vision è­˜åˆ¥çµæœ
    /// - Parameters:
    ///   - observations: Vision è­˜åˆ¥è§€å¯Ÿçµæœ
    ///   - processingTime: è™•ç†æ™‚é–“
    /// - Returns: OCR çµæœ
    private func processObservations(_ observations: [VNRecognizedTextObservation], processingTime: TimeInterval) -> OCRResult {
        var allText: [String] = []
        var boundingBoxes: [TextBoundingBox] = []
        var totalConfidence: Float = 0.0
        
        for observation in observations {
            guard let topCandidate = observation.topCandidates(1).first else { continue }
            
            let text = topCandidate.string
            let confidence = topCandidate.confidence
            
            allText.append(text)
            totalConfidence += confidence
            
            // å–å¾—æ›´å¤šå€™é¸é …ç›®
            let candidates = observation.topCandidates(3).map { $0.string }
            
            // å»ºç«‹é‚Šç•Œæ¡†è³‡è¨Š
            let boundingBox = TextBoundingBox(
                text: text,
                confidence: confidence,
                boundingBox: observation.boundingBox,
                topCandidates: candidates
            )
            
            boundingBoxes.append(boundingBox)
        }
        
        let recognizedText = allText.joined(separator: "\n")
        let averageConfidence = observations.isEmpty ? 0.0 : totalConfidence / Float(observations.count)
        
        print("ğŸ” VisionService: OCR å®Œæˆ")
        print("ğŸ“ è­˜åˆ¥æ–‡å­—é•·åº¦: \(recognizedText.count) å­—å…ƒ")
        print("ğŸ¯ å¹³å‡ä¿¡å¿ƒåº¦: \(String(format: "%.2f", averageConfidence))")
        print("â±ï¸ è™•ç†æ™‚é–“: \(String(format: "%.3f", processingTime)) ç§’")
        print("ğŸ“¦ é‚Šç•Œæ¡†æ•¸é‡: \(boundingBoxes.count)")
        
        #if DEBUG
        print("ğŸ“„ è­˜åˆ¥å…§å®¹é è¦½:")
        print(recognizedText.prefix(200))
        if recognizedText.count > 200 {
            print("... (ç¸½å…± \(recognizedText.count) å­—å…ƒ)")
        }
        #endif
        
        return OCRResult(
            recognizedText: recognizedText,
            confidence: averageConfidence,
            boundingBoxes: boundingBoxes,
            processingTime: processingTime
        )
    }
    
    // MARK: - Utility Methods
    
    /// å¾é‚Šç•Œæ¡†è³‡è¨Šæå–ç‰¹å®šå€åŸŸçš„æ–‡å­—
    /// - Parameters:
    ///   - boundingBoxes: é‚Šç•Œæ¡†é™£åˆ—
    ///   - region: ç›®æ¨™å€åŸŸ (æ­£è¦åŒ–åº§æ¨™ 0.0-1.0)
    /// - Returns: è©²å€åŸŸçš„æ–‡å­—é™£åˆ—
    func extractTextInRegion(_ boundingBoxes: [TextBoundingBox], region: CGRect) -> [String] {
        return boundingBoxes.compactMap { box in
            if box.boundingBox.intersects(region) {
                return box.text
            }
            return nil
        }
    }
    
    /// æ ¹æ“šä¿¡å¿ƒåº¦éæ¿¾æ–‡å­—
    /// - Parameters:
    ///   - boundingBoxes: é‚Šç•Œæ¡†é™£åˆ—
    ///   - minimumConfidence: æœ€å°ä¿¡å¿ƒåº¦é–¾å€¼
    /// - Returns: éæ¿¾å¾Œçš„æ–‡å­—é™£åˆ—
    func filterTextByConfidence(_ boundingBoxes: [TextBoundingBox], minimumConfidence: Float) -> [String] {
        return boundingBoxes.compactMap { box in
            if box.confidence >= minimumConfidence {
                return box.text
            }
            return nil
        }
    }
    
    /// å–å¾—æ”¯æ´çš„èªè¨€åˆ—è¡¨
    /// - Returns: æ”¯æ´çš„èªè¨€ä»£ç¢¼é™£åˆ—
    func getSupportedLanguages() -> [String] {
        return ["en-US", "zh-Hant", "zh-Hans"]
    }
    
    /// æª¢æŸ¥ Vision Framework å¯ç”¨æ€§
    /// - Returns: æ˜¯å¦å¯ç”¨
    func isVisionAvailable() -> Bool {
        if #available(iOS 13.0, *) {
            return true
        } else {
            return false
        }
    }
}

// MARK: - UIImage Extension (åƒè€ƒ Ver2)
extension UIImage {
    /// æ­£è¦åŒ–åœ–ç‰‡æ–¹å‘
    func normalizeOrientation() -> UIImage? {
        if imageOrientation == .up {
            return self
        }
        
        UIGraphicsBeginImageContextWithOptions(size, false, scale)
        draw(in: CGRect(origin: .zero, size: size))
        let normalizedImage = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        
        return normalizedImage
    }
}


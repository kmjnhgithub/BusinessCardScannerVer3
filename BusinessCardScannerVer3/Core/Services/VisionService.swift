//
//  VisionService.swift
//  BusinessCardScannerVer3
//
//  Vision Framework OCR æœå‹™
//

import UIKit
import Vision
import Combine
import CoreImage

/// OCR è­˜åˆ¥çµæœ
struct OCRResult {
    let recognizedText: String
    let confidence: Float
    let boundingBoxes: [TextBoundingBox]
    let processingTime: TimeInterval
}

/// æ–‡å­—é‚Šç•Œæ¡†è³‡è¨Š
struct TextBoundingBox {
    let text: String
    let confidence: Float
    let boundingBox: CGRect
    let topCandidates: [String]
}

/// åç‰‡åµæ¸¬çµæœ
struct CardDetectionResult {
    let observation: VNRectangleObservation
    let croppedImage: UIImage
    let confidence: Float
}

/// å®Œæ•´çš„åç‰‡è™•ç†çµæœ
struct BusinessCardProcessResult {
    let croppedImage: UIImage
    let ocrResult: OCRResult
}

/// Vision æœå‹™éŒ¯èª¤é¡å‹
enum VisionError: LocalizedError {
    case imageProcessingFailed
    case noTextFound
    case visionRequestFailed(Error)
    case invalidImage
    case noRectangleDetected
    case croppingFailed
    case lowConfidence
    
    var errorDescription: String? {
        switch self {
        case .imageProcessingFailed:
            return "åœ–ç‰‡è™•ç†å¤±æ•—"
        case .noTextFound:
            return "æœªæ‰¾åˆ°æ–‡å­—"
        case .visionRequestFailed(let error):
            return "Vision è­˜åˆ¥å¤±æ•—: \(error.localizedDescription)"
        case .invalidImage:
            return "ç„¡æ•ˆçš„åœ–ç‰‡"
        case .noRectangleDetected:
            return "æœªåµæ¸¬åˆ°åç‰‡"
        case .croppingFailed:
            return "è£åˆ‡å¤±æ•—"
        case .lowConfidence:
            return "è­˜åˆ¥ä¿¡å¿ƒåº¦éä½"
        }
    }
}

/// OCR éŒ¯èª¤é¡å‹ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
typealias OCRError = VisionError

/// Vision Framework OCR æœå‹™
class VisionService {
    
    // MARK: - Singleton
    
    static let shared = VisionService()
    private init() {}
    
    // MARK: - Properties
    
    /// OCR è«‹æ±‚é…ç½®
    private var textRecognitionRequest: VNRecognizeTextRequest {
        let request = VNRecognizeTextRequest()
        
        // è¨­å®šè­˜åˆ¥ç­‰ç´šï¼ˆç²¾ç¢ºåº¦ vs é€Ÿåº¦ï¼‰
        request.recognitionLevel = .accurate
        
        // ä½¿ç”¨æœ€æ–°çš„ revision ä»¥ç¢ºä¿ API å®‰å…¨æ€§å’Œå…¼å®¹æ€§
        // ç§»é™¤å·²æ£„ç”¨çš„ supportedRecognitionLanguages æª¢æŸ¥ï¼Œç›´æ¥è¨­å®šæ”¯æ´çš„èªè¨€
        print("ğŸŒ VisionService: ä½¿ç”¨ç³»çµ±é è¨­æ”¯æ´çš„èªè¨€é…ç½®")
        
        // ä¿®æ­£ï¼šä½¿ç”¨ Ver2 çš„æ¢ä»¶å¼èªè¨€è¨­å®šé‚è¼¯
        if #available(iOS 16.0, *) {
            // iOS 16+ æ”¯æ´æ›´å¤šèªè¨€
            request.recognitionLanguages = ["zh-Hant", "en-US"] // ç¹é«”ä¸­æ–‡å’Œè‹±æ–‡
            print("ğŸ¯ iOS 16+ ä½¿ç”¨èªè¨€: [\"zh-Hant\", \"en-US\"]")
        } else {
            // iOS 15 åŠä»¥ä¸‹ç‰ˆæœ¬ä½¿ç”¨è‹±æ–‡
            request.recognitionLanguages = ["en-US"]
            print("ğŸ¯ iOS 15- ä½¿ç”¨èªè¨€: [\"en-US\"]")
        }
        
        // ä½¿ç”¨èªè¨€æ ¡æ­£
        request.usesLanguageCorrection = true
        
        // è¨­å®šæœ€å°æ–‡å­—é«˜åº¦ï¼ˆé™ä½é–¾å€¼ä»¥æé«˜å°å­—è­˜åˆ¥ï¼‰
        request.minimumTextHeight = 0.02
        
        // è¨­å®šè‡ªå‹•è­˜åˆ¥èªè¨€
        request.automaticallyDetectsLanguage = true
        
        return request
    }
    
    // MARK: - Public Methods
    
    /// å°åœ–ç‰‡åŸ·è¡Œ OCR æ–‡å­—è­˜åˆ¥
    /// - Parameters:
    ///   - image: è¦è­˜åˆ¥çš„åœ–ç‰‡
    ///   - completion: å®Œæˆå›èª¿ (Result<OCRResult, OCRError>)
    func recognizeText(from image: UIImage, completion: @escaping (Result<OCRResult, OCRError>) -> Void) {
        let startTime = CFAbsoluteTimeGetCurrent()
        
        // å…ˆæ­£è¦åŒ–åœ–ç‰‡æ–¹å‘ï¼ˆåƒè€ƒ Ver2ï¼‰
        guard let normalizedImage = image.normalizeOrientation(),
              let cgImage = normalizedImage.cgImage else {
            completion(.failure(.invalidImage))
            return
        }
        
        // åœ¨èƒŒæ™¯åŸ·è¡Œç·’è™•ç†
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self = self else { return }
            
            let request = self.textRecognitionRequest
            // ä½¿ç”¨æ­£è¦åŒ–å¾Œçš„åœ–ç‰‡ï¼Œæ–¹å‘ç‚º .up
            let handler = VNImageRequestHandler(cgImage: cgImage, orientation: .up, options: [:])
            
            do {
                try handler.perform([request])
                
                guard let observations = request.results else {
                    DispatchQueue.main.async {
                        completion(.failure(.noTextFound))
                    }
                    return
                }
                
                let result = self.processObservations(observations, processingTime: CFAbsoluteTimeGetCurrent() - startTime)
                
                DispatchQueue.main.async {
                    completion(.success(result))
                }
                
            } catch {
                DispatchQueue.main.async {
                    completion(.failure(.visionRequestFailed(error)))
                }
            }
        }
    }
    
    /// å°åœ–ç‰‡åŸ·è¡Œ OCR æ–‡å­—è­˜åˆ¥ (Combine ç‰ˆæœ¬)
    /// - Parameter image: è¦è­˜åˆ¥çš„åœ–ç‰‡
    /// - Returns: OCR çµæœçš„ Publisher
    func recognizeText(from image: UIImage) -> AnyPublisher<OCRResult, OCRError> {
        return Future { [weak self] promise in
            self?.recognizeText(from: image) { result in
                promise(result)
            }
        }
        .eraseToAnyPublisher()
    }
    
    // MARK: - Private Methods
    
    /// è™•ç† Vision è­˜åˆ¥çµæœ
    /// - Parameters:
    ///   - observations: Vision è­˜åˆ¥è§€å¯Ÿçµæœ
    ///   - processingTime: è™•ç†æ™‚é–“
    /// - Returns: OCR çµæœ
    private func processObservations(_ observations: [VNRecognizedTextObservation], processingTime: TimeInterval) -> OCRResult {
        var allText: [String] = []
        var boundingBoxes: [TextBoundingBox] = []
        var totalConfidence: Float = 0.0
        
        for observation in observations {
            guard let topCandidate = observation.topCandidates(1).first else { continue }
            
            let text = topCandidate.string
            let confidence = topCandidate.confidence
            
            allText.append(text)
            totalConfidence += confidence
            
            // å–å¾—æ›´å¤šå€™é¸é …ç›®
            let candidates = observation.topCandidates(3).map { $0.string }
            
            // å»ºç«‹é‚Šç•Œæ¡†è³‡è¨Š
            let boundingBox = TextBoundingBox(
                text: text,
                confidence: confidence,
                boundingBox: observation.boundingBox,
                topCandidates: candidates
            )
            
            boundingBoxes.append(boundingBox)
        }
        
        let recognizedText = allText.joined(separator: "\n")
        let averageConfidence = observations.isEmpty ? 0.0 : totalConfidence / Float(observations.count)
        
        print("ğŸ” VisionService: OCR å®Œæˆ")
        print("ğŸ“ è­˜åˆ¥æ–‡å­—é•·åº¦: \(recognizedText.count) å­—å…ƒ")
        print("ğŸ¯ å¹³å‡ä¿¡å¿ƒåº¦: \(String(format: "%.2f", averageConfidence))")
        print("â±ï¸ è™•ç†æ™‚é–“: \(String(format: "%.3f", processingTime)) ç§’")
        print("ğŸ“¦ é‚Šç•Œæ¡†æ•¸é‡: \(boundingBoxes.count)")
        
        #if DEBUG
        print("ğŸ“„ è­˜åˆ¥å…§å®¹é è¦½:")
        print(recognizedText.prefix(200))
        if recognizedText.count > 200 {
            print("... (ç¸½å…± \(recognizedText.count) å­—å…ƒ)")
        }
        #endif
        
        return OCRResult(
            recognizedText: recognizedText,
            confidence: averageConfidence,
            boundingBoxes: boundingBoxes,
            processingTime: processingTime
        )
    }
    
    // MARK: - Utility Methods
    
    /// å¾é‚Šç•Œæ¡†è³‡è¨Šæå–ç‰¹å®šå€åŸŸçš„æ–‡å­—
    /// - Parameters:
    ///   - boundingBoxes: é‚Šç•Œæ¡†é™£åˆ—
    ///   - region: ç›®æ¨™å€åŸŸ (æ­£è¦åŒ–åº§æ¨™ 0.0-1.0)
    /// - Returns: è©²å€åŸŸçš„æ–‡å­—é™£åˆ—
    func extractTextInRegion(_ boundingBoxes: [TextBoundingBox], region: CGRect) -> [String] {
        return boundingBoxes.compactMap { box in
            if box.boundingBox.intersects(region) {
                return box.text
            }
            return nil
        }
    }
    
    /// æ ¹æ“šä¿¡å¿ƒåº¦éæ¿¾æ–‡å­—
    /// - Parameters:
    ///   - boundingBoxes: é‚Šç•Œæ¡†é™£åˆ—
    ///   - minimumConfidence: æœ€å°ä¿¡å¿ƒåº¦é–¾å€¼
    /// - Returns: éæ¿¾å¾Œçš„æ–‡å­—é™£åˆ—
    func filterTextByConfidence(_ boundingBoxes: [TextBoundingBox], minimumConfidence: Float) -> [String] {
        return boundingBoxes.compactMap { box in
            if box.confidence >= minimumConfidence {
                return box.text
            }
            return nil
        }
    }
    
    /// å–å¾—æ”¯æ´çš„èªè¨€åˆ—è¡¨
    /// - Returns: æ”¯æ´çš„èªè¨€ä»£ç¢¼é™£åˆ—
    func getSupportedLanguages() -> [String] {
        return ["en-US", "zh-Hant", "zh-Hans"]
    }
    
    /// æª¢æŸ¥ Vision Framework å¯ç”¨æ€§
    /// - Returns: æ˜¯å¦å¯ç”¨
    func isVisionAvailable() -> Bool {
        if #available(iOS 13.0, *) {
            return true
        } else {
            return false
        }
    }
    
    // MARK: - åç‰‡åµæ¸¬èˆ‡è£åˆ‡ (åƒè€ƒ Ver2)
    
    /// åµæ¸¬åœ–ç‰‡ä¸­çš„çŸ©å½¢ï¼ˆåç‰‡ï¼‰
    func detectRectangle(in image: UIImage, completion: @escaping (Result<VNRectangleObservation, VisionError>) -> Void) {
        print("ğŸ” VisionService: é–‹å§‹åµæ¸¬åç‰‡çŸ©å½¢ï¼Œåœ–ç‰‡å°ºå¯¸: \(image.size)")
        
        // å…ˆæ­£è¦åŒ–åœ–ç‰‡æ–¹å‘
        guard let normalizedImage = image.normalizeOrientation(),
              let cgImage = normalizedImage.cgImage else {
            print("âŒ VisionService: åœ–ç‰‡æ­£è¦åŒ–å¤±æ•—")
            completion(.failure(.imageProcessingFailed))
            return
        }
        
        print("ğŸ” VisionService: åœ–ç‰‡æ­£è¦åŒ–å®Œæˆï¼ŒCGImage å°ºå¯¸: \(cgImage.width)x\(cgImage.height)")
        
        let request = VNDetectRectanglesRequest { request, error in
            if let error = error {
                print("ğŸ” VisionService: çŸ©å½¢åµæ¸¬éŒ¯èª¤: \(error)")
                completion(.failure(.noRectangleDetected))
                return
            }
            
            guard let observations = request.results as? [VNRectangleObservation] else {
                print("ğŸ” VisionService: æ²’æœ‰åµæ¸¬çµæœ")
                completion(.failure(.noRectangleDetected))
                return
            }
            
            print("ğŸ” VisionService: åµæ¸¬åˆ° \(observations.count) å€‹çŸ©å½¢å€™é¸")
            
            if observations.isEmpty {
                print("ğŸ” VisionService: çŸ©å½¢åˆ—è¡¨ç‚ºç©º")
                completion(.failure(.noRectangleDetected))
                return
            }
            
            // é¡¯ç¤ºæ‰€æœ‰åµæ¸¬çµæœçš„è©³ç´°è³‡è¨Š
            for (index, obs) in observations.enumerated() {
                print("ğŸ” çŸ©å½¢ \(index + 1): ä¿¡å¿ƒåº¦=\(String(format: "%.3f", obs.confidence))")
            }
            
            // é¸æ“‡ä¿¡å¿ƒåº¦æœ€é«˜çš„çŸ©å½¢
            let sortedObservations = observations.sorted { $0.confidence > $1.confidence }
            if let best = sortedObservations.first {
                print("âœ… VisionService: é¸æ“‡æœ€ä½³çŸ©å½¢ï¼Œä¿¡å¿ƒåº¦: \(String(format: "%.3f", best.confidence))")
                completion(.success(best))
            } else {
                print("âŒ VisionService: ç„¡æ³•æ‰¾åˆ°åˆé©çš„çŸ©å½¢")
                completion(.failure(.noRectangleDetected))
            }
        }
        
        // è¨­å®šåµæ¸¬åƒæ•¸ï¼ˆåƒè€ƒ Ver2 çš„æˆåŠŸç¶“é©—ï¼‰
        request.minimumAspectRatio = 0.3  // åç‰‡å¯èƒ½çš„æœ€å°é•·å¯¬æ¯”
        request.maximumAspectRatio = 0.9  // åç‰‡å¯èƒ½çš„æœ€å¤§é•·å¯¬æ¯”
        request.minimumSize = 0.2         // æœ€å°å°ºå¯¸ï¼ˆç›¸å°æ–¼åœ–ç‰‡ï¼‰
        request.maximumObservations = 3   // æœ€å¤šåµæ¸¬3å€‹çŸ©å½¢
        request.minimumConfidence = 0.5   // æœ€ä½ä¿¡å¿ƒåº¦
        
        print("ğŸ” VisionService: åµæ¸¬åƒæ•¸è¨­å®šå®Œæˆ")
        print("   - é•·å¯¬æ¯”ç¯„åœ: \(request.minimumAspectRatio) ~ \(request.maximumAspectRatio)")
        print("   - æœ€å°å°ºå¯¸: \(request.minimumSize)")
        print("   - æœ€ä½ä¿¡å¿ƒåº¦: \(request.minimumConfidence)")
        print("   - æœ€å¤§è§€å¯Ÿæ•¸: \(request.maximumObservations)")
        
        // åŸ·è¡Œè«‹æ±‚ - ä½¿ç”¨æ­£è¦åŒ–å¾Œçš„åœ–ç‰‡ï¼Œæ–¹å‘ç‚º .up
        let handler = VNImageRequestHandler(cgImage: cgImage, orientation: .up)
        
        DispatchQueue.global(qos: .userInitiated).async {
            do {
                try handler.perform([request])
            } catch {
                print("Vision åŸ·è¡ŒéŒ¯èª¤: \(error)")
                DispatchQueue.main.async {
                    completion(.failure(.noRectangleDetected))
                }
            }
        }
    }
    
    /// æ ¹æ“šåµæ¸¬çµæœè£åˆ‡åç‰‡
    func cropCard(from image: UIImage, observation: VNRectangleObservation, completion: @escaping (Result<UIImage, VisionError>) -> Void) {
        print("âœ‚ï¸ VisionService: é–‹å§‹è£åˆ‡åç‰‡ï¼Œè§€å¯Ÿä¿¡å¿ƒåº¦: \(String(format: "%.3f", observation.confidence))")
        
        // å…ˆå°‡ UIImage æ­£è¦åŒ–ï¼ˆä¿®æ­£æ–¹å‘ï¼‰
        guard let normalizedImage = image.normalizeOrientation(),
              let cgImage = normalizedImage.cgImage else {
            print("âŒ VisionService: è£åˆ‡å‰åœ–ç‰‡æ­£è¦åŒ–å¤±æ•—")
            completion(.failure(.imageProcessingFailed))
            return
        }
        
        print("âœ‚ï¸ VisionService: è£åˆ‡åœ–ç‰‡æ­£è¦åŒ–å®Œæˆï¼Œå°ºå¯¸: \(cgImage.width)x\(cgImage.height)")
        
        DispatchQueue.global(qos: .userInitiated).async {
            // ä½¿ç”¨ Core Image é€²è¡Œé€è¦–æ ¡æ­£å’Œè£åˆ‡
            let ciImage = CIImage(cgImage: cgImage)
            
            // å°‡æ­£è¦åŒ–åº§æ¨™è½‰æ›ç‚ºåœ–ç‰‡åº§æ¨™
            let imageSize = CGSize(width: cgImage.width, height: cgImage.height)
            
            // Vision Framework çš„åº§æ¨™ç³»çµ±ï¼šå·¦ä¸‹è§’ç‚ºåŸé» (0,0)
            // Core Image çš„åº§æ¨™ç³»çµ±ï¼šå·¦ä¸‹è§’ç‚ºåŸé» (0,0)
            // æ‰€ä»¥ä¸éœ€è¦ Y è»¸åè½‰
            let topLeft = CGPoint(x: observation.topLeft.x * imageSize.width,
                                 y: observation.topLeft.y * imageSize.height)
            let topRight = CGPoint(x: observation.topRight.x * imageSize.width,
                                  y: observation.topRight.y * imageSize.height)
            let bottomLeft = CGPoint(x: observation.bottomLeft.x * imageSize.width,
                                    y: observation.bottomLeft.y * imageSize.height)
            let bottomRight = CGPoint(x: observation.bottomRight.x * imageSize.width,
                                     y: observation.bottomRight.y * imageSize.height)
            
            // å»ºç«‹é€è¦–æ ¡æ­£æ¿¾é¡
            guard let perspectiveFilter = CIFilter(name: "CIPerspectiveCorrection") else {
                DispatchQueue.main.async {
                    completion(.failure(.croppingFailed))
                }
                return
            }
            
            perspectiveFilter.setValue(ciImage, forKey: kCIInputImageKey)
            perspectiveFilter.setValue(CIVector(cgPoint: topLeft), forKey: "inputTopLeft")
            perspectiveFilter.setValue(CIVector(cgPoint: topRight), forKey: "inputTopRight")
            perspectiveFilter.setValue(CIVector(cgPoint: bottomLeft), forKey: "inputBottomLeft")
            perspectiveFilter.setValue(CIVector(cgPoint: bottomRight), forKey: "inputBottomRight")
            
            // å–å¾—æ ¡æ­£å¾Œçš„åœ–ç‰‡
            guard let outputImage = perspectiveFilter.outputImage else {
                DispatchQueue.main.async {
                    completion(.failure(.croppingFailed))
                }
                return
            }
            
            // è½‰æ›å› UIImage
            let context = CIContext()
            if let correctedCGImage = context.createCGImage(outputImage, from: outputImage.extent) {
                // ä½¿ç”¨æ­£å¸¸æ–¹å‘å»ºç«‹ UIImage
                let correctedUIImage = UIImage(cgImage: correctedCGImage)
                print("âœ… VisionService: é€è¦–æ ¡æ­£æˆåŠŸï¼Œè£åˆ‡å¾Œå°ºå¯¸: \(correctedUIImage.size)")
                DispatchQueue.main.async {
                    completion(.success(correctedUIImage))
                }
            } else {
                print("âŒ VisionService: é€è¦–æ ¡æ­£å¾Œç„¡æ³•å»ºç«‹ UIImage")
                DispatchQueue.main.async {
                    completion(.failure(.croppingFailed))
                }
            }
        }
    }
    
    /// å®Œæ•´çš„åç‰‡è™•ç†æµç¨‹ï¼šåµæ¸¬ â†’ è£åˆ‡ â†’ OCR
    func processBusinessCard(image: UIImage, completion: @escaping (Result<BusinessCardProcessResult, VisionError>) -> Void) {
        print("ğŸ¯ VisionService: é–‹å§‹å®Œæ•´åç‰‡è™•ç†æµç¨‹")
        
        // Step 1: åµæ¸¬åç‰‡
        detectRectangle(in: image) { [weak self] rectangleResult in
            guard let self = self else { return }
            
            switch rectangleResult {
            case .success(let observation):
                print("âœ… VisionService: Step 1 å®Œæˆ - åç‰‡åµæ¸¬æˆåŠŸ")
                
                // Step 2: è£åˆ‡åç‰‡
                self.cropCard(from: image, observation: observation) { cropResult in
                    switch cropResult {
                    case .success(let croppedImage):
                        print("âœ… VisionService: Step 2 å®Œæˆ - åç‰‡è£åˆ‡æˆåŠŸï¼Œå°ºå¯¸: \(croppedImage.size)")
                        
                        // Step 3: OCR è­˜åˆ¥
                        self.recognizeText(from: croppedImage) { ocrResult in
                            switch ocrResult {
                            case .success(let ocr):
                                print("âœ… VisionService: Step 3 å®Œæˆ - OCR è­˜åˆ¥æˆåŠŸ")
                                let result = BusinessCardProcessResult(croppedImage: croppedImage, ocrResult: ocr)
                                completion(.success(result))
                            case .failure(let error):
                                print("âŒ VisionService: Step 3 å¤±æ•— - OCR è­˜åˆ¥å¤±æ•—: \(error.localizedDescription)")
                                completion(.failure(error))
                            }
                        }
                    case .failure(let error):
                        print("âŒ VisionService: Step 2 å¤±æ•— - åç‰‡è£åˆ‡å¤±æ•—: \(error.localizedDescription)")
                        completion(.failure(error))
                    }
                }
            case .failure(let error):
                // å¦‚æœåµæ¸¬å¤±æ•—ï¼Œç›´æ¥å°åŸåœ–é€²è¡Œ OCR
                print("âš ï¸ VisionService: Step 1 å¤±æ•— - çŸ©å½¢åµæ¸¬å¤±æ•—ï¼Œæ”¹ç”¨åŸåœ– OCR")
                self.recognizeText(from: image) { ocrResult in
                    switch ocrResult {
                    case .success(let ocr):
                        print("âœ… VisionService: åŸåœ– OCR æˆåŠŸ - ä½¿ç”¨åŸåœ–ä½œç‚ºçµæœ")
                        // è¿”å›åŸåœ–å’Œ OCR çµæœ
                        let result = BusinessCardProcessResult(croppedImage: image, ocrResult: ocr)
                        completion(.success(result))
                    case .failure:
                        print("âŒ VisionService: åŸåœ– OCR ä¹Ÿå¤±æ•—")
                        completion(.failure(error))
                    }
                }
            }
        }
    }
    
    /// å–å¾—åœ–ç‰‡æ–¹å‘
    private func imageOrientation(from image: UIImage) -> CGImagePropertyOrientation {
        switch image.imageOrientation {
        case .up:
            return .up
        case .down:
            return .down
        case .left:
            return .left
        case .right:
            return .right
        case .upMirrored:
            return .upMirrored
        case .downMirrored:
            return .downMirrored
        case .leftMirrored:
            return .leftMirrored
        case .rightMirrored:
            return .rightMirrored
        @unknown default:
            return .up
        }
    }
}

// MARK: - UIImage Extension (åƒè€ƒ Ver2)
extension UIImage {
    /// æ­£è¦åŒ–åœ–ç‰‡æ–¹å‘
    func normalizeOrientation() -> UIImage? {
        if imageOrientation == .up {
            return self
        }
        
        UIGraphicsBeginImageContextWithOptions(size, false, scale)
        draw(in: CGRect(origin: .zero, size: size))
        let normalizedImage = UIGraphicsGetImageFromCurrentImageContext()
        UIGraphicsEndImageContext()
        
        return normalizedImage
    }
}

